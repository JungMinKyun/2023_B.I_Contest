{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["Import Library"],"metadata":{"id":"aHPIB1sRLVt3"}},{"cell_type":"code","source":["import numpy as np\n","import pandas as pd\n","import datetime\n","import os\n","import copy\n","\n","import matplotlib.pyplot as plt\n","\n","# Scaling\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# CustomDataSet\n","from torch.utils.data import DataLoader, Dataset\n","\n","# Transformer\n","import math\n","import torch\n","from torch import nn, Tensor\n","import torch.optim as optim\n","from torch.nn import TransformerEncoder, TransformerEncoderLayer\n","\n","from tqdm import tqdm"],"metadata":{"id":"kDJBdPIWLUxB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Load Data"],"metadata":{"id":"mfYW02fqLYLo"}},{"cell_type":"code","source":["data = pd.read_csv( '/data_selected.csv', index_col = 0 )\n","data.index = pd.to_datetime( data.index )\n","\n","data_dae = pd.read_csv( '/DAE_result.csv', index_col = 0 )\n","data_dae.index = pd.to_datetime( data.index )\n","\n","data_DAE = copy.deepcopy( data )\n","data_DAE['현재수요+태양광'] = data_dae['DAE']\n","data_DAE"],"metadata":{"id":"HPSBL_NGKa-q"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Transformer"],"metadata":{"id":"enr-W-6rLaMR"}},{"cell_type":"markdown","source":["Setting"],"metadata":{"id":"iyfkNANIL0Nx"}},{"cell_type":"code","source":["\"\"\"\n","ip : 입력 데이터 기간\n","op : 출력 데이터 기간\n","stride : stride\n","batch : 배치 사이즈\n","num_indep : 독립 변수 갯수\n","mode : scalar 종류, min-max & standard\n","n_head : Attention head 갯수\n","d_model : 임베딩 차원\n","n_layer : 레이어 갯수\n","lr : 학습률\n","epoch : 학습 횟수\n","patience_limit : early stop 조정 파라미터\n","dropout_TF : 인코더 레이어 dropout 비율\n","dropout_PE : Positional Encoding dropout 비율\n","train_start : 학습 데이터 시작 일시\n","val_start : 검증 데이터 시작 일시\n","test_start : 테스트 데이터 시작 일시\n","test_last : 테스트 데이터 마지막 일시\n","\"\"\"\n","\n","\n","Setting = {\n","    'ip' : 720, 'op' : 72, 'stride' : 72,\n","    'batch' : 32,\n","    'num_indep' : len( data.columns ) - 3,\n","    'mode' : 'standard', # min-max / standard\n","    'n_head' : len( data.columns ) - 3,\n","    'd_model' : ( len( data.columns ) - 3 )*31,\n","    'n_layer' : 2,\n","    'lr' : 1e-4,\n","    'epoch' : 150,\n","    'patience_limit' : 7,\n","    'dropout_TF' : 0.1,\n","    'dropout_PE' : 0.1,\n","    'train_start' : '2022-03-01 00:00:00',\n","    'val_start' : '2023-02-01 00:00:00',\n","    'test_start' : '2023-03-13 00:00:00',\n","    'test_last' : '2023-03-19 23:55:00'\n","    }"],"metadata":{"id":"W_POMsBtLd6E"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"ACqZouUPaV-T"}},{"cell_type":"markdown","source":["Transform to float32"],"metadata":{"id":"D_qI3wT_f9T-"}},{"cell_type":"code","source":["# 데이터 타입을 float32로 바꿔주는 함수\n","\n","def Transform_to_float32( data, num ) :\n","\n","    # data : 데이터\n","    # num : 바꿀 변수 갯수\n","\n","    for i in range( num ) :\n","        col = data.columns[ i ]\n","        data[ col ] = data[ col ].astype( np.float32 )\n","\n","    return data"],"metadata":{"id":"sReeMLtmf_tu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Seed 고정"],"metadata":{"id":"zVge7jZoiu_O"}},{"cell_type":"code","source":["# Seed 고정 함수\n","\n","def seed_everything(seed: int = 42):\n","\n","    np.random.seed(seed)\n","    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = True"],"metadata":{"id":"fAMr79RUi3VE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Split_train_test_set"],"metadata":{"id":"0jL41m26agRl"}},{"cell_type":"code","source":["# Train set, Validation set, Test set 분할 함수\n","\n","def split_train_val_test(input_df, train_start, val_start, test_start, test_last, ip) :\n","\n","    # input_df : 입력 데이터\n","    # train_start : 학습 데이터 시작 일시\n","    # val_start : 검증 데이터 시작 일시\n","    # test_start : 테스트 데이터 시작 일시\n","    # test_last : 테스트 데이터 마지막 일시\n","    # ip : 가중치 업데이트에 사용되는 입력 데이터 길이\n","\n","    import copy\n","    from datetime import datetime, timedelta\n","\n","    copy_df = copy.deepcopy(input_df)\n","\n","    date_format = '%Y-%m-%d %H:%M:%S'\n","\n","    train_date = datetime.strptime(train_start, date_format)\n","    val_date = datetime.strptime(val_start, date_format)\n","    test_date = datetime.strptime(test_start, date_format)\n","\n","    train = copy_df[train_date : val_date - timedelta(minutes = 5 * ip) - timedelta(minutes = 5)]\n","    val = copy_df[val_date - timedelta(minutes = 5 * ip) : test_date - timedelta(minutes = 5 * ip) - timedelta(minutes = 5) ]\n","    test = copy_df[test_date  - timedelta(minutes = 5 * ip) : test_last]\n","\n","    return train, val, test"],"metadata":{"id":"Sc1IvMbcabgf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Scaling"],"metadata":{"id":"XiAtE2m0amDE"}},{"cell_type":"code","source":["# Scaling 함수 : MinMax / Standard\n","\n","def data_scaling(X_train, X_val, X_test, method) :\n","    global min_max, std\n","\n","    # X_train : 학습 데이터 셋\n","    # X_val : 검증 데이터 셋\n","    # X_test : 테스트 데이터 셋\n","    # method : min-max, standard\n","\n","    drop_list = ['공휴일', '주말']\n","    drop_idx = 7\n","\n","    temp_X_train = X_train[drop_list]\n","    temp_X_val = X_val[drop_list]\n","    temp_X_test = X_test[drop_list]\n","\n","    X_train = X_train.drop(columns = drop_list, axis = 1)\n","    X_val = X_val.drop(columns = drop_list, axis = 1)\n","    X_test = X_test.drop(columns = drop_list, axis = 1)\n","\n","    if method == 'standard':\n","\n","        from sklearn.preprocessing import StandardScaler\n","\n","        std = StandardScaler()\n","        std.fit(X_train)\n","\n","        X_train_scaled = std.transform(X_train)\n","        X_val_scaled = std.transform(X_val)\n","        X_test_scaled = std.transform(X_test)\n","\n","        X_train_scaled_df = pd.DataFrame(index = X_train.index, data = X_train_scaled, columns = X_train.columns)\n","        X_train_scaled_df = pd.concat([X_train_scaled_df.iloc[:, : drop_idx ], temp_X_train, X_train_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        X_val_scaled_df = pd.DataFrame(index = X_val.index, data = X_val_scaled, columns = X_train.columns)\n","        X_val_scaled_df = pd.concat([X_val_scaled_df.iloc[:, : drop_idx ], temp_X_val, X_val_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        X_test_scaled_df = pd.DataFrame(index = X_test.index, data = X_test_scaled, columns = X_train.columns)\n","        X_test_scaled_df = pd.concat([X_test_scaled_df.iloc[:, : drop_idx ], temp_X_test, X_test_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        return X_train_scaled_df, X_val_scaled_df, X_test_scaled_df\n","\n","    elif method == 'min-max':\n","\n","        from sklearn.preprocessing import MinMaxScaler\n","\n","        min_max = MinMaxScaler()\n","        min_max.fit(X_train)\n","\n","        X_train_scaled = min_max.transform(X_train)\n","        X_val_scaled = min_max.transform(X_val)\n","        X_test_scaled = min_max.transform(X_test)\n","\n","        X_train_scaled_df = pd.DataFrame(index = X_train.index, data = X_train_scaled, columns = X_train.columns)\n","        X_train_scaled_df = pd.concat([X_train_scaled_df.iloc[:, : drop_idx ], temp_X_train, X_train_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        X_val_scaled_df = pd.DataFrame(index = X_val.index, data = X_val_scaled, columns = X_train.columns)\n","        X_val_scaled_df = pd.concat([X_val_scaled_df.iloc[:, : drop_idx ], temp_X_val, X_val_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        X_test_scaled_df = pd.DataFrame(index = X_test.index, data = X_test_scaled, columns = X_train.columns)\n","        X_test_scaled_df = pd.concat([X_test_scaled_df.iloc[:, : drop_idx ], temp_X_test, X_test_scaled_df.iloc[:, drop_idx : ]], axis = 1)\n","\n","        return X_train_scaled_df, X_val_scaled_df, X_test_scaled_df"],"metadata":{"id":"6AGQdUsBanUJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Make CustomDataSet for PyTorch"],"metadata":{"id":"8qn_9YzuaqcA"}},{"cell_type":"code","source":["# DataSet 만드는 함수\n","\n","class CustomDataSet(Dataset):\n","    def __init__(self, DataSet, ip, op, stride):\n","\n","        # DataSet : 입력 데이터\n","        # ip : input size\n","        # op : output size\n","        # stride : 데이터 간격\n","\n","        self.DataSet = DataSet\n","        self.ip = ip\n","        self.op = op\n","        self.stride = stride\n","        self.data_list = []\n","        self.label_list = []\n","        self.indep_list = []\n","\n","        print('Data Pre-processing..')\n","\n","        L = len(DataSet)\n","        num_samples = (L - ip - op) // stride + 1\n","\n","        self.input_df = DataSet['현재수요+태양광']\n","        self.indep_variable = DataSet.drop( columns = [ '태양광 발전량(MWh)', '현재수요(MW)', '현재수요+태양광' ],axis = 1 )\n","        self.target_df = DataSet['현재수요+태양광']\n","\n","        for idx in tqdm(range(num_samples)):\n","\n","            Start_x = stride * idx\n","            End_x = Start_x + ip\n","            temp_time_data = self.input_df[Start_x : End_x]\n","            temp_inpet_variable = self.indep_variable[Start_x : End_x]\n","\n","            self.data_list.append(torch.Tensor(temp_time_data.values))\n","            self.indep_list.append( torch.Tensor( temp_inpet_variable.values ) )\n","\n","            Start_y = stride * idx + ip\n","            End_y = Start_y + op\n","            temp_label_data = self.target_df[Start_y : End_y]\n","            self.label_list.append(torch.Tensor(temp_label_data.values))\n","\n","        print(\"DONE!!\\n\")\n","\n","    def __getitem__(self, index):\n","        data = self.data_list[index]\n","        label = self.label_list[index]\n","        indep = self.indep_list[index]\n","\n","        return data, label, indep\n","\n","    def __len__(self):\n","        return len(self.data_list)"],"metadata":{"id":"TSs47RiUas40"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Transformer"],"metadata":{"id":"jfEEA_zuauFH"}},{"cell_type":"code","source":["# Transformer Model\n","\n","class TFModel( nn.Module ) :\n","\n","    def __init__( self, iw, ow, d_model, nhead, nlayers, dropout = 0.5 ) :\n","\n","        # iw : input size                 | 720\n","        # ow : output size                | 72\n","        # d_model : 임베딩 차원           | 279( 9*31 )\n","        # nhead : Attention head 갯수     | 9\n","        # nlayers : 레이어 갯수           | 2\n","        # dropout : dropout 비율          | 0.1\n","\n","        d_model = d_model + Setting['num_indep']\n","        new_d_model = d_model - Setting['num_indep']\n","\n","        super( TFModel, self ).__init__()\n","        self.encoder_layer = nn.TransformerEncoderLayer( d_model = d_model, nhead = nhead, dropout = dropout )\n","        self.transformer_encoder = nn.TransformerEncoder( self.encoder_layer, num_layers = nlayers )\n","        self.pos_encoder = PositionalEncoding( d_model, dropout )\n","\n","        # 입력데이터를 임베딩하여 d_model 차원으로 확장\n","        self.encoder = nn.Sequential(\n","            nn.Linear( 1, new_d_model//2 ),\n","            nn.ReLU(),\n","            nn.Linear( new_d_model//2, new_d_model )\n","        )\n","\n","        # transformer의 출력을 다시 시계열 데이터의 예측값으로 변환\n","        self.linear = nn.Sequential(\n","            nn.Linear( d_model, d_model//2 ),\n","            nn.ReLU(),\n","            nn.Linear( d_model//2, 1 )\n","        )\n","\n","        # 입력 차원인 iw에서 출력 차원인 ow로 변환\n","        self.linear2 = nn.Sequential(\n","            nn.Linear( iw, ( iw + ow )//2 ),\n","            nn.ReLU(),\n","            nn.Linear( ( iw + ow )//2, ow )\n","        )\n","\n","    # 마스크 생성 함수\n","    def generate_square_subsequent_mask( self, sz ) :\n","\n","        mask = ( torch.triu( torch.ones( sz,sz ) ) == 1 ).transpose(0,1)\n","        mask = mask.float().masked_fill( mask == 0, float( '-inf' ) ).masked_fill( mask == 1, float(0.0) )\n","\n","        return mask\n","\n","    # src : 입력 시계열 데이터, srcmask : 입력시계열 데이터에 대한 마스크\n","    def forward( self, src, srcmask, indeps ) :\n","\n","        src = self.encoder( src )\n","        src = torch.cat( (src, indeps), dim = 2 )\n","        src = self.pos_encoder( src )\n","        output = self.transformer_encoder( src.transpose(0,1), srcmask ).transpose( 0,1 )\n","        output = self.linear( output )[:,:,0]\n","        output = self.linear2( output )\n","\n","        return output"],"metadata":{"id":"2P4HEXPKaz8T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 위치정보 인코딩 클래스\n","\n","class PositionalEncoding( nn.Module ) :\n","\n","    def __init__( self, d_model, dropout = 0.1, max_len = 5000 ) :\n","\n","        # d_model : 임베딩 차원\n","        # dropout : dropout 비율\n","        # max_len : Positional Matrix 초기 사이즈\n","\n","        super( PositionalEncoding, self ).__init__()\n","        self.dropout = nn.Dropout( p = dropout )\n","\n","        d_model_new = d_model\n","\n","        pe = torch.zeros( max_len, d_model_new )\n","\n","        position = torch.arange( 0, max_len, dtype = torch.float ).unsqueeze(1)\n","        div_term = torch.exp( torch.arange(0, d_model_new, 2).float() * (-math.log( 10000.0 ) / d_model_new ) )\n","\n","        pe[:, 0::2] = torch.sin( position * div_term )\n","        pe[:, 1::2] = torch.cos( position * div_term )\n","        pe = pe.unsqueeze(0).transpose(0,1)\n","        self.register_buffer( 'pe', pe )\n","\n","    def forward( self, x ) :\n","\n","        x = x + self.pe[:x.size(0), :]\n","\n","        return self.dropout(x)\n"],"metadata":{"id":"zkJ5B_Kca3g_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# mask 행렬 반환 함수\n","\n","def gen_attention_mask(x) :\n","    mask = torch.eq(x, 0)\n","\n","    return mask"],"metadata":{"id":"3zlVdqIpa4SG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Train & Validation & Test"],"metadata":{"id":"MQmCPDr9a5nb"}},{"cell_type":"code","source":["# 학습 및 검증 함수\n","\n","def Train_Validation( Settings, train, validation, test ) :\n","\n","    # Settings : 하이퍼파라미터 설정값\n","    # train : 학습 데이터\n","    # validation : 검증 데이터\n","    # test : 테스트 데이터\n","\n","    device = torch.device( 'cuda' )\n","\n","    Setting = Settings\n","    train_loader = train\n","    val_loader = validation\n","    test_loader = test\n","\n","    best_loss = 10**9\n","    patience_limit = Setting['patience_limit']\n","    patience_check = 0\n","\n","    lr = Setting['lr']\n","    model = TFModel( iw = Setting['ip'], ow = Setting['op'], d_model = Setting['d_model'], nhead = Setting['n_head'], nlayers = Setting['n_layer'], dropout = Setting['dropout_TF'] ).to(device) # nhead : 멀티헤드어텐션의 개수\n","    criterion = nn.MSELoss()\n","    optimizer = torch.optim.Adam( model.parameters(), lr=lr )\n","\n","    # Sceduler\n","    scheduler = optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n","                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n","                                        last_epoch=-1,\n","                                        verbose=False)\n","\n","\n","    train_loss_per_epoch = []\n","    val_loss_per_epoch = []\n","    test_loss_per_epoch = []\n","\n","    epoch = Setting['epoch']\n","    progress = tqdm( range(epoch) )\n","    for i in progress :\n","\n","        ### Train\n","        batchloss = 0.0\n","        model.train()\n","        for ( inputs, outputs, indeps ) in train_loader :\n","            optimizer.zero_grad()\n","            src_mask = model.generate_square_subsequent_mask( inputs.shape[1] ).to(device)\n","            result = model( inputs.unsqueeze(-1).float().to(device), src_mask, indeps.float().to(device) )\n","            loss = criterion( result, outputs.float().to(device) )\n","            loss.backward()\n","            batchloss += loss\n","            optimizer.step()\n","\n","        scheduler.step()\n","\n","        train_loss_per_epoch.append( batchloss.cpu().item() / len( train_loader ) )\n","\n","        ### Validation\n","        val_loss = 0.0\n","        model.eval()\n","        for ( inputs, outputs, indeps ) in val_loader :\n","\n","            src_mask = model.generate_square_subsequent_mask( inputs.shape[1] ).to(device) # 굳이 안해줘도 됐을 듯\n","            Y_pred = model( inputs.unsqueeze(-1).float().to(device), src_mask, indeps.float().to(device) )\n","            loss = criterion( Y_pred, outputs.float().to(device) )\n","            val_loss += loss\n","        val_loss_per_epoch.append( val_loss.cpu().item() / len( val_loader ) )\n","\n","        ### Test\n","        test_loss = 0.0\n","        model.eval()\n","        for ( inputs, outputs, indeps ) in test_loader :\n","\n","            src_mask = model.generate_square_subsequent_mask( inputs.shape[1] ).to(device) # 굳이 안해줘도 됐을 듯\n","            result = model( inputs.unsqueeze(-1).float().to(device), src_mask, indeps.float().to(device) )\n","            loss = criterion( result, outputs.float().to(device) )\n","            test_loss += loss\n","        test_loss_per_epoch.append( test_loss.cpu().item() / len( test_loader ) )\n","\n","        ### Early Stopping\n","        if ( val_loss > best_loss ) :\n","            patience_check += 1\n","\n","            if ( patience_check >= patience_limit ) :\n","                print( '\\nEarly Stopped!' )\n","\n","                break\n","\n","        else :\n","            best_loss = val_loss\n","            best_model = model\n","            patience_check = 0\n","\n","    return train_loss_per_epoch, val_loss_per_epoch, test_loss_per_epoch, best_model"],"metadata":{"id":"o1VccEJObEhd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Test & Flatten Results"],"metadata":{"id":"GFrAuR_IbI64"}},{"cell_type":"code","source":["# 테스트 결과 수집 함수\n","\n","def Get_test_result(model, test_loader, device):\n","\n","    # model : 학습된 모델\n","    # test_loader : 테스트 데이터 로더\n","    # device : device\n","\n","    pred_result = []\n","\n","    model = model\n","\n","    for X, Y, Z in tqdm(iter(test_loader)):\n","\n","        src_mask = model.generate_square_subsequent_mask( X.shape[1] ).to(device)\n","        model_pred = model( X.unsqueeze(-1).float().to(device), src_mask, Z.float().to(device) )\n","\n","\n","        for i in model_pred.flatten():\n","\n","            pred_result.append(i.item())\n","\n","    return pred_result"],"metadata":{"id":"zdXuUKebbhLE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Inverse Transformation of Results"],"metadata":{"id":"6gC7I1iIbkj9"}},{"cell_type":"code","source":["# Scale 역변환 함수 : 정규화된 데이터 값을 원래 단위 값으로 변환\n","\n","def Inverse_scalar_transformation(input_df, scalar, scalar_type) :\n","\n","    # input_df : 입력 데이터\n","    # scalar_type : 스케일러 종류 (min-max, standard)\n","\n","    real_pred_result = []\n","\n","    if scalar_type == 'min-max':\n","\n","        min = scalar.data_min_[-1]\n","        max = scalar.data_max_[-1]\n","\n","        for i in input_df:\n","\n","            temp_value = min + i * (max-min)\n","\n","            real_pred_result.append(temp_value)\n","\n","        real_pred_result = pd.Series(real_pred_result)\n","\n","\n","    if scalar_type == 'standard':\n","\n","        mean = scalar.mean_[-1]\n","        std = np.sqrt(scalar.var_[-1])\n","\n","        for i in input_df:\n","\n","            temp_value = mean + i * (std)\n","            real_pred_result.append(temp_value)\n","\n","        real_pred_result = pd.Series(real_pred_result)\n","\n","    return real_pred_result"],"metadata":{"id":"Ih5ExjERboHn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["MAPE"],"metadata":{"id":"9OtUFHQ7bp0x"}},{"cell_type":"code","source":["# MAPE 계산 함수\n","\n","def MAPE(y_test, y_pred):\n","\n","    # y_test : 실제 목표변수 값\n","    # y_pred : 예측된 목표변수 값\n","\n","\treturn np.mean(np.abs((y_test - y_pred) / y_test)) * 100"],"metadata":{"id":"_kUz2OpwbrKe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### MAIN"],"metadata":{"id":"4-ewghcYbs9q"}},{"cell_type":"code","source":["# Setting에 따른 Transformer 모델 실행\n","\n","def Transformer_Scenario( Settings, datas ) :\n","\n","    # Settings : 하이퍼파라미터 설정값\n","    # datas : 데이터\n","\n","    device = torch.device( 'cuda' )\n","\n","    seed_everything(42)\n","\n","    Setting = Settings\n","    data = datas\n","\n","    ### Transform to float32\n","    data = Transform_to_float32( data, Setting['num_indep'] )\n","\n","    ### Split_train_val_test\n","    data_train, data_val, data_test =  split_train_val_test( data, Setting['train_start'], Setting['val_start'], Setting['test_start'], Setting['test_last'], ip = Setting['ip'] )\n","\n","    ### Scaling\n","    data_train_scaled, data_val_scaled, data_test_scaled = data_scaling( data_train, data_val, data_test, Setting['mode'] )\n","\n","    ### Make CustomDataSet\n","    train_dataset = CustomDataSet(data_train_scaled, ip = Setting['ip'], op = Setting['op'], stride = Setting['stride'])\n","    train_loader = DataLoader(train_dataset, batch_size = Setting['batch'], shuffle=False)\n","\n","    val_dataset = CustomDataSet(data_val_scaled, ip = Setting['ip'], op = Setting['op'], stride = Setting['stride'])\n","    val_loader = DataLoader(val_dataset, batch_size = Setting['batch'], shuffle = False)\n","\n","    test_dataset = CustomDataSet(data_test_scaled, ip = Setting['ip'], op = Setting['op'], stride = Setting['stride'])\n","    test_loader = DataLoader(test_dataset, batch_size = Setting['batch'], shuffle = False)\n","\n","    ### Train & Validation & Test\n","    train_loss_per_epoch, val_loss_per_epoch, test_loss_per_epoch, best_model = Train_Validation( Setting, train_loader, val_loader, test_loader )\n","\n","    ### Test & Flatten Results\n","    pred_result = Get_test_result( best_model, test_loader, device )\n","\n","    ### Inverse Transformation of Results\n","    if Setting['mode'] == 'min-max' :\n","        Scaler = min_max\n","    else :\n","        Scaler = std\n","    real_pred_result = Inverse_scalar_transformation( pred_result, Scaler, Setting['mode'] )\n","\n","    ### MAPE\n","    Y_test = data_test['현재수요(MW)']['2023-03-13 00:00:00' : '2023-03-19 23:55:00']\n","    Y_test_reset_index = Y_test.reset_index(drop = True)\n","    Y_test_reset_index = Y_test_reset_index.to_numpy()\n","\n","    Sunpower_test= data_test['태양광 발전량(MWh)']['2023-03-13 00:00:00' : '2023-03-19 23:55:00']\n","    Sunpower_test_reset_index = Sunpower_test.reset_index(drop = True)\n","\n","    real_test_result = real_pred_result - Sunpower_test_reset_index\n","    real_test_result = real_test_result.to_numpy()\n","\n","    mape = MAPE( Y_test, real_test_result )\n","\n","    ### 결과 저장\n","    result_dic = {\n","        'best_model' : best_model,\n","        'train_loss_per_epoch' : train_loss_per_epoch,\n","        'val_loss_per_epoch' : val_loss_per_epoch,\n","        'test_loss_per_epoch' : test_loss_per_epoch,\n","        'Y_test' : Y_test,\n","        'real_test_result' : real_test_result,\n","        'mape' : mape\n","        }\n","\n","    return result_dic\n","\n"],"metadata":{"id":"_e5ZgyTbLzKA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###실행"],"metadata":{"id":"oCx29fYRtwYM"}},{"cell_type":"code","source":["result_dic = Transformer_Scenario( Setting, data_DAE )"],"metadata":{"id":"dctK_AYEkFjx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result_dic['mape']"],"metadata":{"id":"5ox-NVNUtuv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 결과 저장"],"metadata":{"id":"hoLoNlkQTH3n"}},{"cell_type":"code","source":["submit = pd.DataFrame( data = result_dic['real_test_result'], columns = ['예측된 현재수요(MW)'], index = result_dic['Y_test'].index )\n","submit.to_csv('저장경로/submit.csv')"],"metadata":{"id":"GJA1ln-CTJ6t"},"execution_count":null,"outputs":[]}]}